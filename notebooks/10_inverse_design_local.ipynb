{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Using the local generator to do the inverse design\n",
    "output-file: inverse_design_local.html\n",
    "title: Inverse Design Local\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inverse_design_local_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "\n",
    "from inverse_design.inverse_design import *\n",
    "from inverse_design.brushes import notched_square_brush\n",
    "from inverse_design.naive_inverse_design import (\n",
    "    viz_sim, mode_overlap, mask_combine_epsr, init_domain\n",
    ")\n",
    "from inverse_design.conditional_generator import (\n",
    "    new_latent_design, transform\n",
    ")\n",
    "from tqdm.notebook import trange\n",
    "from jax.example_libraries.optimizers import adam\n",
    "from ceviche.modes import insert_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from javiche import jaxit\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from inverse_design.local_generator import generate_feasible_design_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook overwrites the inverse design notebook to use the local_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angular frequency of the source in Hz\n",
    "omega = 2 * np.pi * 200e12\n",
    "# Spatial resolution in meters\n",
    "dl = 30e-9\n",
    "# Number of pixels in x-direction\n",
    "Nx = 120\n",
    "# Number of pixels in y-direction\n",
    "Ny = 120\n",
    "# Number of pixels in the PMLs in each direction\n",
    "Npml = 20\n",
    "# Initial value of the structure's relative permittivity\n",
    "epsr_init = 12.0\n",
    "# Space between the PMLs and the design region (in pixels)\n",
    "space = 10\n",
    "# Width of the waveguide (in pixels)\n",
    "wg_width = 12\n",
    "# Length in pixels of the source/probe slices on each side of the center point\n",
    "space_slice = 8\n",
    "# Number of epochs in the optimization\n",
    "Nsteps = 150\n",
    "# Step size for the Adam optimizer\n",
    "def step_size(idx):\n",
    "  \"\"\"reducing the stepsize linearly for Nsteps (stabilize afterwards just in case)\"\"\"\n",
    "  return np.maximum((5e-3)**(idx/Nsteps), 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsr, bg_epsr, design_region, input_slice, output_slice = init_domain(\n",
    "    Nx, Ny, Npml, space=space, wg_width=wg_width, space_slice=space_slice\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsr_total = mask_combine_epsr(epsr, bg_epsr, design_region)\n",
    "\n",
    "# Setup source\n",
    "source = insert_mode(omega, dl, input_slice.x, input_slice.y, epsr_total, m=1)\n",
    "\n",
    "# Setup probe\n",
    "probe = insert_mode(omega, dl, output_slice.x, output_slice.y, epsr_total, m=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brush = notched_square_brush(5, 1)\n",
    "latent = new_latent_design((Nx, Ny), r=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate initial device\n",
    "simulation, ax = viz_sim(epsr_total, source, slices=[input_slice, output_slice])\n",
    "_, _, Ez = simulation.solve(source)\n",
    "E0 = mode_overlap(Ez, probe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverse_design.local_generator import dilate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_mask = np.logical_or(bg_epsr>2, design_region)\n",
    "eroded = dilate(np.logical_not(bg_mask), brush)\n",
    "dilated = dilate(bg_epsr>2, brush)\n",
    "\n",
    "init_t_s = np.logical_not(np.logical_or(eroded, design_region)) \n",
    "init_t_v = np.logical_not(np.logical_or(dilated, design_region)) \n",
    "# plt.imshow(init_t_s, vmax=1, vmin=0)\n",
    "# plt.figure()\n",
    "# plt.imshow(init_t_v, vmax=1, vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_t = transform(latent, brush)\n",
    "generate_feasible_design_mask(\n",
    "  latent_t, brush, init_touches_solid=init_t_s.copy(), \n",
    "  init_touches_void=init_t_v.copy(), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(latent_weights, brush):\n",
    "    latent_t = transform(latent_weights, brush) #.reshape((Nx, Ny))\n",
    "    design_mask = generate_feasible_design_mask(latent_t, \n",
    "      brush, init_touches_solid=init_t_s, init_touches_void=init_t_v, verbose=False)\n",
    "    epsr = (design_mask+1.0)/2.0*(12-1) +1 \n",
    "    # complicated expression to avoid where clause, as it caused problems with differentiation\n",
    "    # why did the np.where clause lead to 0 gradients?\n",
    "    return epsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jaxit(cache=True)\n",
    "def inner_loss_fn(epsr):\n",
    "    #print(\".\")\n",
    "    simulation.eps_r = mask_combine_epsr(epsr, bg_epsr, design_region)\n",
    "    _, _, Ez = simulation.solve(source)\n",
    "\n",
    "    return -mode_overlap(Ez, probe) / E0\n",
    "\n",
    "def loss_fn(latent):\n",
    "    epsr = forward(latent, brush)\n",
    "    # def debug_plot(epsr):\n",
    "    #   plt.figure(figsize=(0.5,0.5))\n",
    "    #   plt.imshow(epsr)\n",
    "    #   plt.axis(\"off\")\n",
    "    #   plt.show()\n",
    "    # jax.debug.callback(debug_plot, epsr)\n",
    "    return inner_loss_fn(epsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "grad_fn = jax.jacfwd(generate_feasible_design_mask)\n",
    "grads = grad_fn(latent_t, brush)\n",
    "assert grads.std() != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "grad_fn = jax.jacfwd(forward)\n",
    "grads = grad_fn(latent, brush)\n",
    "assert grads.std() != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "grad_fn = jax.value_and_grad(loss_fn)\n",
    "loss, grad = grad_fn(latent)\n",
    "assert grad.std() != 0\n",
    "assert loss != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn, update_fn, params_fn = adam(step_size)\n",
    "state = init_fn(latent) #.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the optimization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_fn(step, state):\n",
    "    latent = params_fn(state) # we need autograd arrays here...\n",
    "    loss, grads = grad_fn(latent)\n",
    "    #loss = loss_fn(latent)\n",
    "    optim_state = update_fn(step, grads, state)\n",
    "    optim_latent = params_fn(optim_state)\n",
    "    optim_latent = optim_latent/optim_latent.std()\n",
    "    return loss, init_fn(optim_latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now loop over the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code-fold:true\n",
    "range_ = trange(Nsteps)\n",
    "losses = np.ndarray(Nsteps)\n",
    "for step in range_:\n",
    "    loss, state = step_fn(step, state)\n",
    "    losses[step] = loss\n",
    "    range_.set_postfix(loss=float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and show the optimal device\n",
    "epsr_optimum = forward(params_fn(state), brush)\n",
    "epsr_optimum_total = mask_combine_epsr(epsr_optimum, bg_epsr, design_region)\n",
    "simulation, ax = viz_sim(epsr_optimum_total, source, slices=[input_slice, output_slice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"step number\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "y",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
